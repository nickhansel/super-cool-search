{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyNP7Bu9gx0dIxY5V8Mj5cg2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Fs_Fd3O1pikt"},"outputs":[],"source":["!pip install \"grpcio<=1.58.0,>=1.49.1\" pinecone-client sentence-transformers tqdm nltk numpy"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"uE-IYvHAps6b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pinecone\n","import uuid\n","from sentence_transformers import SentenceTransformer\n","from nltk.tokenize import word_tokenize\n","import nltk\n","import json\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import numpy as np\n","import sys\n","import os"],"metadata":{"id":"T-uiysIbpwdp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"id":"dllYoF1pp4Ij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import zipfile\n","import os\n","\n","def unzip_file(zip_path, extract_to=\"./unzipped_data\"):\n","    if not os.path.exists(extract_to):\n","        os.makedirs(extract_to)\n","\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_to)\n","        print(f\"Extracted all files to {extract_to}\")\n","\n","unzip_file(\"./drive/MyDrive/data.zip\")"],"metadata":{"id":"hiWWbwiCp4OL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = SentenceTransformer(\n","    \"all-MiniLM-L6-v2\", device=\"cuda\"\n",")"],"metadata":{"id":"TurqLSCLp6GX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Embedder:\n","    def __init__(self, dir_paths, pinecone_api_key, collection_name, env, batch_size=30000, model='all-MiniLM-L6-v2'):\n","        self.dir_paths = dir_paths\n","        self.batch_size = batch_size\n","        self.model_name = model\n","        self.collection_name = collection_name\n","        self.init_pinecone(pinecone_api_key, env)\n","        self.record_limit = 300000\n","\n","    def init_pinecone(self, api_key, env):\n","        pinecone.init(api_key=api_key, environment=env)\n","\n","        # Delete the index if it already exists\n","        if self.collection_name in pinecone.list_indexes():\n","            print(f\"deleting pinecone index: {self.collection_name}\")\n","            pinecone.delete_index(self.collection_name)\n","\n","        # Create the index\n","        print(f\"creating index: {self.collection_name}\")\n","        pinecone.create_index(self.collection_name, dimension=384)\n","        self.index = pinecone.Index(self.collection_name)\n","        print(f\"successfully created pinecone index: {self.collection_name}\")\n","\n","\n","    def tokenize_text(self, text):\n","        return word_tokenize(text)\n","\n","    def embed_text(self, text_batch):\n","        model = SentenceTransformer(self.model_name, device=\"cuda\")\n","        return model.encode(text_batch, show_progress_bar=True)\n","\n","    def insert_embeddings(self, embedding_data, json_data):\n","        max_batch_size = 1000  # max vectors per pinecone upsert\n","\n","        with ThreadPoolExecutor(max_workers=20) as executor:\n","            futures = []\n","            for i in range(0, len(embedding_data), max_batch_size):\n","                batch_embeddings = embedding_data[i:i + max_batch_size]\n","                batch_json = json_data[i:i + max_batch_size]\n","\n","                future = executor.submit(self.process_and_insert_batch, batch_embeddings, batch_json)\n","                futures.append(future)\n","\n","            for future in futures:\n","                future.result()\n","\n","    def gen_ids(self, num_ids):\n","        return [str(uuid.uuid4()) for _ in range(num_ids)]\n","\n","    def embeddings_to_list(self, embeddings):\n","        return [embedding.tolist() if isinstance(embedding, np.ndarray) else embedding for embedding in embeddings]\n","\n","    def process_and_insert_batch(self, embeddings, json_data, max_batch_size_bytes=2 * 1024 * 1024): # 2MB\n","        # generate unique id for each embedding\n","        ids = self.gen_ids(len(embeddings))\n","\n","        # convert embeddings from np arrays to list\n","        embeddings_list = self.embeddings_to_list(embeddings)\n","\n","        current_batch = []\n","        current_batch_size = 0\n","\n","        for id, embedding, metadata in zip(ids, embeddings_list, json_data):\n","            serialized_metadata = self.jsonify_metadata(metadata)\n","            if not serialized_metadata:\n","                continue\n","\n","            item = (id, embedding, json.loads(serialized_metadata))\n","            item_size = sys.getsizeof(json.dumps(item))\n","\n","            if current_batch_size + item_size > max_batch_size_bytes:\n","                self.index.upsert(vectors=current_batch)\n","                current_batch = [item]\n","                current_batch_size = item_size\n","            else:\n","                current_batch.append(item)\n","                current_batch_size += item_size\n","\n","        if current_batch:\n","            self.index.upsert(vectors=current_batch)\n","            print(f\"Successfully inserted {len(current_batch)} vectors and documents into Pinecone.\")\n","\n","    def jsonify_metadata(self, metadata):\n","        try:\n","            metadata_dict = json.loads(metadata) if isinstance(metadata, str) else metadata\n","\n","            serialized_metadata = json.dumps(metadata_dict)\n","            if sys.getsizeof(serialized_metadata) > 40960: # max pinecone upsert limit\n","                return None\n","\n","            return serialized_metadata\n","        except json.JSONDecodeError:\n","            print(f\"failed to encode json: {metadata}\")\n","            return None\n","\n","    def batchify(self, texts):\n","        for i in range(0, len(texts), self.batch_size):\n","            yield texts[i:i + self.batch_size]\n","\n","    def join_line(self, lines):\n","        res = []\n","        for line in lines:\n","            title = json.loads(line).get('title', '')\n","            selftext = json.loads(line).get('selftext', '')\n","            body = json.loads(line).get('body', '')\n","            tokenized_text = ' '.join(self.tokenize_text(title + selftext + body))\n","            res.append(tokenized_text)\n","        return res\n","\n","\n","    def process_lines(self, file_path):\n","        with open(file_path, 'r', encoding='utf-8') as file:\n","            count = 0\n","            while count < self.record_limit:\n","                lines = [line.strip() for line in file]\n","                if not lines:\n","                    break\n","                texts = self.join_line(lines)\n","                embeddings = self.embed_text(texts)\n","                self.insert_embeddings(embeddings, lines)\n","                count += len(lines)\n","                print(f\"inserted {len(lines)} vectors from {file_path} into pinecone\")\n","\n","    def run(self):\n","        print(\"Starting Embedder...\")\n","        for dir_path in self.dir_paths:\n","            print(f\"Processing directory: {dir_path}\")\n","            file_paths = [os.path.join(dir_path, f) for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n","            for file_path in file_paths:\n","                self.process_lines(file_path)\n","\n","\n","if __name__ == '__main__':\n","    pinecone_api_key = ''\n","    env = ''\n","    collection_name = 'test-search'\n","    embedder = Embedder([\"./unzipped_data/sample_data/posts\", \"./unzipped_data/sample_data/comments\"], pinecone_api_key, collection_name, env)\n","    embedder.run()\n"],"metadata":{"id":"W57tYpGwpwnc"},"execution_count":null,"outputs":[]}]}